{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import *\n",
    "from Training.UnifiedTrainer import UnifiedTrainer\n",
    "from Training.utils import init_training_components, init_LSTM_with_attention, init_LSTM_without_attention, visualize_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('WELFake_Dataset_processed.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_texts, temp_text, train_labels, temp_labels = train_test_split(\n",
    "  df['full_text_processed'],\n",
    "  df['label'],\n",
    "  random_state=2018,\n",
    "  test_size=0.4,\n",
    "  stratify=df['label']\n",
    ")\n",
    "val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
    "  temp_text,\n",
    "  temp_labels,\n",
    "  random_state=2018,\n",
    "  test_size=0.5,\n",
    "  stratify=temp_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from Training.utils import makeWords\n",
    "\n",
    "mod = gensim.models.Word2Vec(sentences=makeWords(df['full_text_processed']), vector_size=EMBEDDING_DIM, window=5, min_count=1, workers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from NewsDatasetLSTM import NewsDatasetLSTM\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "max_len = 623 \n",
    "# Instantiate datasets\n",
    "train_dataset = NewsDatasetLSTM(train_texts, train_labels, mod, max_len=max_len)\n",
    "val_dataset = NewsDatasetLSTM(val_texts, val_labels, mod, max_len=max_len)\n",
    "test_dataset = NewsDatasetLSTM(test_texts, test_labels, mod, max_len=max_len)\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size,drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_negatives = len(df[df['label'] == 0])\n",
    "num_positives = len(df[df['label'] == 1])\n",
    "class_counts = [num_negatives, num_positives]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Training.CheckpointManager import CheckpointManager\n",
    "\n",
    "checkpoint_manager = CheckpointManager(\n",
    "\t\tsave_dir='checkpoints',\n",
    "\t\tsave_name='LSTMWithAttention'\n",
    "\t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM with Attention results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = init_LSTM_with_attention().to(DEVICE)\n",
    "\n",
    "optimizer1, scheduler1 = init_training_components(model1)\n",
    "\n",
    "trainer1 = UnifiedTrainer(\n",
    "\tmodel=model1,\n",
    "\toptimizer=optimizer1,\n",
    "\tclass_counts=class_counts,\n",
    "\tscheduler=scheduler1,\n",
    "\tdevice=DEVICE,\n",
    "\tgrad_clip=1.0,\n",
    "\tearly_stopping_patience=EARLY_STOP_PATIENCE,\n",
    "\tsave_name = 'LSTMWithAttention'\n",
    "\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint loaded from checkpoints\\LSTMWithAttention_final.pt\n",
      "Resuming from epoch 10\n"
     ]
    }
   ],
   "source": [
    "epoch, metrics, hyperparams, metadata, early_stopping_state = (\n",
    "\ttrainer1.checkpoint_manager.load_checkpoint(\n",
    "\tmodel=model1,\n",
    "\toptimizer=optimizer1,\n",
    "\tscheduler=scheduler1,\n",
    "\tload_type='final')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing with best model checkpoint...\n",
      "Checkpoint loaded from checkpoints\\LSTMWithAttention_best.pt\n",
      "Resuming from epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|\u001b[34m██████████\u001b[0m| 194/194 [00:25<00:00,  7.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Results:\n",
      "Loss: 0.2789\n",
      "Accuracy: 0.8848\n",
      "F1 Score: 0.8820\n",
      "Precision: 0.8011\n",
      "Recall: 0.9812\n",
      "ROC AUC: 0.9707\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_results = trainer1.test(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = init_LSTM_without_attention().to(DEVICE)\n",
    "\n",
    "optimizer2, scheduler2 = init_training_components(model2)\n",
    "\n",
    "trainer2 = UnifiedTrainer(\n",
    "\tmodel=model2,\n",
    "\toptimizer=optimizer2,\n",
    "\tclass_counts=class_counts,\n",
    "\tscheduler=scheduler2,\n",
    "\tdevice=DEVICE,\n",
    "\tgrad_clip=1.0,\n",
    "\tearly_stopping_patience=EARLY_STOP_PATIENCE,\n",
    "\tsave_name = 'LSTMWithoutAttention'\n",
    "\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint loaded from checkpoints\\LSTMWithoutAttention_final.pt\n",
      "Resuming from epoch 10\n"
     ]
    }
   ],
   "source": [
    "epoch2, metrics2, hyperparams2, metadata2, early_stopping_state2 = (\n",
    "\ttrainer2.checkpoint_manager.load_checkpoint(\n",
    "\tmodel=model2,\n",
    "\toptimizer=optimizer2,\n",
    "\tscheduler=scheduler2,\n",
    "\tload_type='final')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing with best model checkpoint...\n",
      "Checkpoint loaded from checkpoints\\LSTMWithoutAttention_best.pt\n",
      "Resuming from epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|\u001b[34m██████████\u001b[0m| 194/194 [00:18<00:00, 10.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Results:\n",
      "Loss: 0.7627\n",
      "Accuracy: 0.6789\n",
      "F1 Score: 0.7171\n",
      "Precision: 0.5847\n",
      "Recall: 0.9270\n",
      "ROC AUC: 0.7321\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_results2 = trainer2.test(test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
